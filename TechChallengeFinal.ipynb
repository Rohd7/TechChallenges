{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb2b21cd367b478fb6b84cb1fa57b7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82688c7d56184975a2389dd1f1ee7345",
              "IPY_MODEL_b202bc6f41d641bfbc6028a8cea5f83d",
              "IPY_MODEL_9c309b461bfc4abbb071c04fb05a9dc3"
            ],
            "layout": "IPY_MODEL_0201913c77734361ab0aab418756c29c"
          }
        },
        "82688c7d56184975a2389dd1f1ee7345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1deecdc1ff524ff086fe37f5705152dd",
            "placeholder": "​",
            "style": "IPY_MODEL_88517f5f43de40f6b10cd00ed7c0aa87",
            "value": "Map: 100%"
          }
        },
        "b202bc6f41d641bfbc6028a8cea5f83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbbf7b8ea19b45b0af7bdae10c951f86",
            "max": 40631,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b7392247e15435d8563b0310b84cf09",
            "value": 40631
          }
        },
        "9c309b461bfc4abbb071c04fb05a9dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f243b25f0cd64da7bf671d42a8620501",
            "placeholder": "​",
            "style": "IPY_MODEL_09951a7d4e7c45faae8db7f725e15bd6",
            "value": " 40631/40631 [00:07&lt;00:00, 4661.00 examples/s]"
          }
        },
        "0201913c77734361ab0aab418756c29c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1deecdc1ff524ff086fe37f5705152dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88517f5f43de40f6b10cd00ed7c0aa87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbbf7b8ea19b45b0af7bdae10c951f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7392247e15435d8563b0310b84cf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f243b25f0cd64da7bf671d42a8620501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09951a7d4e7c45faae8db7f725e15bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Bloco 1 - Configuração Inicial e Importação de Bibliotecas\n",
        "Descrição:\n",
        "Este bloco monta o Google Drive para permitir acesso aos arquivos armazenados na nuvem durante a execução no Google Colab. Em seguida, importa as bibliotecas essenciais para o projeto:\n",
        "\n",
        "json para manipulação de arquivos JSON,\n",
        "\n",
        "datasets para trabalhar com datasets estruturados no formato HuggingFace,\n",
        "\n",
        "transformers para importar modelos pré-treinados, tokenizadores e utilitários para treinamento."
      ],
      "metadata": {
        "id": "hwux8uKa2eFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importar bibliotecas\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEipMbv2LJ-o",
        "outputId": "1372ee1e-0129-491f-90d5-39442992dc1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloco 2 - Funções Utilitárias para Carregamento e Filtragem de Dados\n",
        "Descrição:\n",
        "Aqui são definidas funções auxiliares fundamentais para o processamento dos dados:\n",
        "\n",
        "load_labels: carrega um conjunto de labels (rótulos) a partir de um arquivo .txt, eliminando linhas vazias.\n",
        "\n",
        "stream_json_lines: lê arquivos JSON no formato JSON Lines (um objeto JSON por linha) de forma streaming, para economizar memória.\n",
        "\n",
        "filtro_por_label: filtra itens JSON verificando se um campo de label (pode ser label, code ou title) pertence a um conjunto de labels pré-definido. Essas funções facilitam o carregamento e filtragem seletiva dos dados que serão usados no treinamento."
      ],
      "metadata": {
        "id": "Zvi1wUmP2hnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para carregar labels do arquivo txt\n",
        "def load_labels(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return set(line.strip() for line in f if line.strip())\n",
        "\n",
        "# Função para ler JSON Lines em streaming\n",
        "def stream_json_lines(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                yield json.loads(line)\n",
        "\n",
        "# Função para filtrar item baseado no label\n",
        "def filtro_por_label(item, label_set):\n",
        "    label = item.get('label') or item.get('code') or item.get('title')\n",
        "    return label in label_set\n"
      ],
      "metadata": {
        "id": "Q5iLevUCNjLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloco 3 - Carregamento, Filtragem e Preparação do Dataset para Fine-Tuning\n",
        "Descrição:\n",
        "Este bloco realiza a carga e filtragem dos dados principais para o fine-tuning:\n",
        "\n",
        "Define funções para ler arquivos JSON Lines completos e para carregar labels de filtro que podem conter múltiplos rótulos por linha.\n",
        "\n",
        "Filtra os datasets de treino e teste usando os labels carregados, garantindo que somente os exemplos relevantes sejam mantidos para treinamento e avaliação.\n",
        "\n",
        "Constrói exemplos no formato esperado para fine-tuning, onde para cada item filtrado é criado um par input (prompt contendo a pergunta sobre o título) e content (descrição do produto).\n",
        "\n",
        "Por fim, imprime informações resumidas do volume de dados carregados e filtrados, preparando o dataset para o próximo passo.\n",
        "\n"
      ],
      "metadata": {
        "id": "mc0UxpOm2ri2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Função para carregar arquivos JSON Lines\n",
        "def load_json_lines(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                obj = json.loads(line)\n",
        "                data.append(obj)\n",
        "    return data\n",
        "\n",
        "# Função para carregar labels a partir dos arquivos de filtro (múltiplos labels por linha)\n",
        "def load_labels_multi_per_line(file_path):\n",
        "    label_set = set()\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            labels = line.strip().split()\n",
        "            label_set.update(labels)  # adiciona todas as labels da linha\n",
        "    return label_set\n",
        "\n",
        "# Função para verificar se qualquer target_ind do item está no label_set\n",
        "def filtro_por_label(item, label_set):\n",
        "    target_inds = item.get('target_ind') or []\n",
        "    target_inds = map(str, target_inds)  # garante que todos são strings\n",
        "    return any(label in label_set for label in target_inds)\n",
        "\n",
        "# Carregar labels de filtro\n",
        "train_labels = load_labels_multi_per_line('/content/drive/MyDrive/Colab Notebooks/Datasets/filter_labels_train.txt')\n",
        "test_labels = load_labels_multi_per_line('/content/drive/MyDrive/Colab Notebooks/Datasets/filter_labels_test.txt')\n",
        "\n",
        "print(f\"Labels de treino carregadas: {len(train_labels)}\")\n",
        "print(f\"Labels de teste carregadas: {len(test_labels)}\")\n",
        "\n",
        "# Carregar datasets\n",
        "trn_data = load_json_lines('/content/drive/MyDrive/Colab Notebooks/Datasets/trn.json')\n",
        "tst_data = load_json_lines('/content/drive/MyDrive/Colab Notebooks/Datasets/tst.json')\n",
        "\n",
        "print(f\"Total de registros no dataset de treino: {len(trn_data)}\")\n",
        "print(f\"Total de registros no dataset de teste: {len(tst_data)}\")\n",
        "\n",
        "# Filtrar datasets conforme labels\n",
        "trn_data_filtrado = [item for item in trn_data if filtro_por_label(item, train_labels)]\n",
        "tst_data_filtrado = [item for item in tst_data if filtro_por_label(item, test_labels)]\n",
        "\n",
        "print(f\"Itens de treino após filtro: {len(trn_data_filtrado)}\")\n",
        "print(f\"Itens de teste após filtro: {len(tst_data_filtrado)}\")\n",
        "\n",
        "# Preparar exemplos para fine-tuning\n",
        "train_examples = []\n",
        "for item in trn_data_filtrado:\n",
        "    title = item.get('title')\n",
        "    content = item.get('content')\n",
        "    if title and content:\n",
        "        pergunta = f\"Quais as informações sobre o produto '{title}'?\"\n",
        "        input_text = f\"Pergunta: {pergunta} Título: {title}\"\n",
        "        target_text = content\n",
        "        train_examples.append({\"input\": input_text, \"target\": target_text})\n",
        "\n",
        "print(f\"Total de exemplos preparados para o treino: {len(train_examples)}\")\n"
      ],
      "metadata": {
        "id": "BI_JtY0BNmzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloco 4 - Setup do Ambiente, Tokenização, Configuração e Execução do Fine-Tuning com LoRA\n",
        "Descrição:\n",
        "Este bloco:\n",
        "\n",
        "Instala pacotes necessários (peft, accelerate, transformers, datasets) para fine-tuning eficiente.\n",
        "\n",
        "Seleciona uma pequena amostra (~3%) do dataset para treinar rapidamente uma versão de teste do modelo.\n",
        "\n",
        "Prepara o dataset para o HuggingFace Trainer, fazendo tokenização e criação dos inputs/labels para modelo causal de linguagem.\n",
        "\n",
        "Configura o modelo TinyLlama-1.1B-Chat-v1.0 para fine-tuning usando a técnica LoRA (Low-Rank Adaptation), que reduz custo computacional ajustando menos parâmetros.\n",
        "\n",
        "Define argumentos de treinamento com parâmetros como batch size, número de passos, taxa de aprendizado, etc., focando em um treinamento rápido e com poucas épocas.\n",
        "\n",
        "Executa o processo de fine-tuning usando o Trainer da HuggingFace.\n",
        "\n",
        "Salva o modelo e tokenizer treinados para uso posterior."
      ],
      "metadata": {
        "id": "R1casEcd32X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft accelerate transformers datasets --quiet"
      ],
      "metadata": {
        "id": "U2pFQZP0ae1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Supondo que train_examples já esteja disponível\n",
        "\n",
        "# Selecionar 3% do dataset para treino rápido\n",
        "total_examples = len(train_examples)\n",
        "subset_size = max(1, int(total_examples * 0.03))  # pelo menos 1 exemplo\n",
        "train_subset = random.sample(train_examples, subset_size)\n",
        "\n",
        "print(f\"Total exemplos para treino: {total_examples}\")\n",
        "print(f\"Usando subset de: {subset_size} exemplos para treinamento\")\n",
        "\n",
        "# Criar Dataset HuggingFace\n",
        "dataset = Dataset.from_list(train_subset)\n",
        "\n",
        "# Carregar tokenizer e modelo\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token  # evitar erro de padding\n",
        "\n",
        "max_seq_length = 128  # reduzido para acelerar\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"{inp} {tgt}\" for inp, tgt in zip(examples[\"input\"], examples[\"target\"])]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_seq_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    # Labels = input_ids para causal LM\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function, batched=True, remove_columns=[\"input\", \"target\"]\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./tinyllama_lora_finetuned\",\n",
        "    per_device_train_batch_size=4,  # Se sua VRAM aguentar, até 8\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=1,             # Uma só época\n",
        "    max_steps=100,                  # Limita para 100 passos de treino!\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=50,\n",
        "    learning_rate=5e-4,             # Mais agressivo para convergir rápido\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=10,                # Pouco warmup\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./tinyllama\")\n",
        "tokenizer.save_pretrained(\"./tinyllama\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "fb2b21cd367b478fb6b84cb1fa57b7d9",
            "82688c7d56184975a2389dd1f1ee7345",
            "b202bc6f41d641bfbc6028a8cea5f83d",
            "9c309b461bfc4abbb071c04fb05a9dc3",
            "0201913c77734361ab0aab418756c29c",
            "1deecdc1ff524ff086fe37f5705152dd",
            "88517f5f43de40f6b10cd00ed7c0aa87",
            "cbbf7b8ea19b45b0af7bdae10c951f86",
            "6b7392247e15435d8563b0310b84cf09",
            "f243b25f0cd64da7bf671d42a8620501",
            "09951a7d4e7c45faae8db7f725e15bd6"
          ]
        },
        "id": "Txg1BYKVarBd",
        "outputId": "e2fa4b6a-9098-497b-ece2-d9b57fe8d238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total exemplos para treino: 1354376\n",
            "Usando subset de: 40631 exemplos para treinamento\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/40631 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb2b21cd367b478fb6b84cb1fa57b7d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-ae00e95efb9f>:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:20, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.887200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.680200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tinyllama/tokenizer_config.json',\n",
              " './tinyllama/special_tokens_map.json',\n",
              " './tinyllama/tokenizer.model',\n",
              " './tinyllama/added_tokens.json',\n",
              " './tinyllama/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Hiperparâmetros utilizados na geração de texto\n",
        "1. max_length=100\n",
        "Define o máximo total de tokens (entrada + saída).\n",
        "\n",
        "Exemplo: se entrada = 50 tokens → só restam 50 tokens para a resposta.\n",
        "\n",
        "Alternativa recomendada: max_new_tokens=50 → controla apenas a quantidade de tokens gerados.\n",
        "\n",
        "2. do_sample=False\n",
        "Geração determinística: sempre escolhe o token mais provável → respostas previsíveis.\n",
        "\n",
        "Método: greedy decoding (sem aleatoriedade).\n",
        "\n",
        "Alternativa:\n",
        "\n",
        "do_sample=True → ativa sampling → respostas mais criativas.\n",
        "\n",
        "3. Outras opções úteis (não usadas mas recomendadas):\n",
        "temperature=0.7 → mais criatividade (quanto maior, mais variado).\n",
        "\n",
        "top_p=0.9 → nucleus sampling → considera só os tokens mais prováveis até acumular 90% de chance.\n",
        "\n",
        "repetition_penalty=1.2 → evita repetições.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yl38b0ol7H_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloco 5 - Comparação entre Modelo Base e Modelo Fine-Tunado\n",
        "Descrição:\n",
        "Este bloco é destinado a carregar o modelo base e o modelo fine-tunado para comparação de performance:\n",
        "\n",
        "Para fins de teste, geramos dois modelos fine-tunados. Um com menos treinamento e outro com mais treinamento, depois que fiz a compra do colab pro conseguimos utilizar maquinas melhores e acelerar o treinamento.\n",
        "\n",
        "Importa os três modelos (base e fine-tunados) com seus respectivos tokenizadores.\n",
        "\n",
        "Cria pipelines de geração de texto para ambos.\n",
        "\n",
        "Monta um exemplo de pergunta usando um título de produto.\n",
        "\n",
        "Gera respostas com os dois modelos para o mesmo prompt.\n",
        "\n",
        "Exibe as respostas lado a lado para facilitar a comparação de melhorias no conteúdo gerado após o fine-tuning, destacando ganhos em relevância e precisão.\n",
        "\n"
      ],
      "metadata": {
        "id": "CSy_CbqL4gHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Caminhos para os modelos\n",
        "caminho_modelo_finetunado = \"/content/tinyllama\"\n",
        "nome_modelo_base = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Carregar tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(nome_modelo_base)\n",
        "\n",
        "# Carregar modelo base\n",
        "modelo_base = AutoModelForCausalLM.from_pretrained(nome_modelo_base)\n",
        "\n",
        "# Carregar modelo finetunado\n",
        "modelo_finetunado = AutoModelForCausalLM.from_pretrained(caminho_modelo_finetunado)\n",
        "\n",
        "# Criar pipelines\n",
        "pipeline_base = pipeline(\"text-generation\", model=modelo_base, tokenizer=tokenizer, device=0)\n",
        "pipeline_finetunado = pipeline(\"text-generation\", model=modelo_finetunado, tokenizer=tokenizer, device=0)\n",
        "\n",
        "# Gerar respostas\n",
        "resposta_base_raw = pipeline_base(texto_entrada, max_length=100, do_sample=False)[0]['generated_text']\n",
        "resposta_finetunada_raw = pipeline_finetunado(texto_entrada, max_length=100, do_sample=False)[0]['generated_text']\n",
        "\n",
        "# Função para extrair apenas a parte gerada após o input\n",
        "def extrair_resposta(texto_entrada, resposta_raw):\n",
        "    return resposta_raw[len(texto_entrada):].strip()\n",
        "\n",
        "# Extrair respostas limpas\n",
        "resposta_base = extrair_resposta(texto_entrada, resposta_base_raw)\n",
        "resposta_finetunada = extrair_resposta(texto_entrada, resposta_finetunada_raw)\n",
        "\n",
        "##############################COLOQUE O TITULO DO PRODUTO AQUI######################\n",
        "exemplo_titulo = \"Girls Ballet Tutu Neon Pink\"\n",
        "pergunta = f\"What are the Information about the product '{exemplo_titulo}'?\"\n",
        "texto_entrada = f\"Pergunta: {pergunta}\"\n",
        "\n",
        "# Exibir respostas\n",
        "print(\"=== Resposta do Modelo Base ===\")\n",
        "print(resposta_base)\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "print(\"=== Resposta do Modelo Finetunado ===\")\n",
        "print(resposta_finetunada)\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "print(\"=== Observações ===\")\n",
        "print(\"Observe possíveis melhorias em relevância, detalhes ou precisão na resposta do modelo finetunado.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktB4mjo8p9p1",
        "outputId": "95718768-ccbd-44ac-d4a8-0cbd51145378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Resposta do Modelo Base ===\n",
            "- Resposta: The Girls Ballet Tutu Neon Pink is a beautiful and elegant tutu dress that is perfect for ballet performances. It features a beautiful neon pink tutu with a delicate lace overlay. The tutu is made of high-quality fabric and is designed to be comfortable and breathable. The tutu is\n",
            "\n",
            "========================================\n",
            "\n",
            "=== Resposta do Modelo Finetunado ===\n",
            "Título: Girls Ballet Tutu Neon Pink Girls Ballet Tutu Neon Pink is a beautifully designed tutu with a neon pink tutu and a matching tutu skirt. The tutu is made of a soft, stretchy fabric and has a beautifully designed tutu skirt. The tutu sk\n",
            "\n",
            "========================================\n",
            "\n",
            "=== Observações ===\n",
            "Observe possíveis melhorias em relevância, detalhes ou precisão na resposta do modelo finetunado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloco 6 - Interface Interativa para Testes com o Modelo Fine-Tunado\n",
        "\n",
        "Descrição:\n",
        "Este último bloco implementa uma interface simples e interativa no terminal para testar o modelo fine-tunado:\n",
        "\n",
        "Instala bibliotecas necessárias para inferência.\n",
        "\n",
        "Carrega o tokenizer e modelo fine-tunado previamente salvo.\n",
        "\n",
        "Cria uma pipeline para geração de texto com o modelo.\n",
        "\n",
        "Define uma função para gerar respostas para qualquer pergunta recebida.\n",
        "\n",
        "Em loop, permite ao usuário digitar perguntas sobre produtos em inglês (compatível com dados de teste) e imprime as respostas do modelo.\n",
        "\n",
        "O usuário pode encerrar o programa digitando \"sair\".\n",
        "\n",
        "Essa etapa serve para validar e demonstrar a aplicabilidade prática do modelo treinado."
      ],
      "metadata": {
        "id": "_7i9Mlcl58jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências necessárias\n",
        "!pip install transformers accelerate peft\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVN4s-VZwlEl",
        "outputId": "76a13023-bce3-4623-b1ed-e3f3e583cb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importar bibliotecas\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Caminho para o modelo fine-tuned menos treinado\n",
        "caminho_modelo_afinadado = \"/content/drive/MyDrive/Colab Notebooks/tiny/content/tinyllama\"\n",
        "\n",
        "# Caminho para o modelo fine-tuned mais treinado\n",
        "caminho_modelo_finetunado = \"/content/drive/MyDrive/Colab Notebooks/loralhama\"\n",
        "\n",
        "# Nome do modelo base\n",
        "nome_modelo_base = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Carregar tokenizer e modelo afinadado\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(caminho_modelo_afinadado)\n",
        "modelo_afinadado = AutoModelForCausalLM.from_pretrained(caminho_modelo_afinadado)\n",
        "\n",
        "# Carregar tokenizer e modelo fine-tuned\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(caminho_modelo_finetunado)\n",
        "modelo_finetunado = AutoModelForCausalLM.from_pretrained(caminho_modelo_finetunado)\n",
        "\n",
        "# Criar pipeline 1\n",
        "pipeline_afinadado = pipeline(\"text-generation\", model=modelo_afinadado, tokenizer=tokenizer1, device=0)\n",
        "\n",
        "\n",
        "# Criar pipeline 2\n",
        "pipeline_finetunado = pipeline(\"text-generation\", model=modelo_finetunado, tokenizer=tokenizer2, device=0)\n",
        "\n",
        "# Função para gerar resposta\n",
        "def responder1(titulo):\n",
        "    entrada = f\"{titulo}\"\n",
        "    resposta = pipeline_afinadado(entrada, max_length=150, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "    return resposta\n",
        "\n",
        "def responder2(titulo):\n",
        "    entrada = f\"{titulo}\"\n",
        "    resposta = pipeline_finetunado(entrada, max_length=150, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "    return resposta\n",
        "\n",
        "# Interface interativa\n",
        "print(\"Modelo carregado com sucesso! Faça perguntas sobre produtos Em ingles para combinar com dados de teste.\\n\")\n",
        "\n",
        "while True:\n",
        "    titulo = input(\"Digite a pergunta do produto (ou 'sair' para encerrar): \")\n",
        "    if titulo.lower() == 'sair':\n",
        "        break\n",
        "    resposta1 = responder1(titulo)\n",
        "    resposta2 = responder2(titulo)\n",
        "    print(\"\\nPergunta Realizada:\",titulo,\"\\n\")\n",
        "    print(\"\\nResposta do modelo 1:\\n\")\n",
        "    print(resposta1)\n",
        "    print(\"\\nResposta do modelo 2:\\n\")\n",
        "    print(resposta2)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E7FS4Swt5mY",
        "outputId": "3741977f-c4b4-4ebb-85eb-baca643ed143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo carregado com sucesso! Faça perguntas sobre produtos Em ingles para combinar com dados de teste.\n",
            "\n",
            "Digite a pergunta do produto (ou 'sair' para encerrar): Whats is the tutu baller \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pergunta Realizada: Whats is the tutu baller  \n",
            "\n",
            "\n",
            "Resposta do modelo 1:\n",
            "\n",
            "Whats is the tutu baller 1303251673113393-1-1303251673113393-1-1303251673113393-1-1303251673113393-1-1303251673113393-1-1303251673113393-1-1303251673113393-1-1303251\n",
            "\n",
            "Resposta do modelo 2:\n",
            "\n",
            "Whats is the tutu baller 101?\n",
            "Design a tutorial using the latest technology.\n",
            "Analyze the impact of technology on the creative process.\n",
            "Design a virtual reality environment.\n",
            "Develop and test a virtual reality experience.\n",
            "Design a virtual reality experience for a consumer product.\n",
            "Create a digital art piece using virtual reality.\n",
            "Design a virtual reality game for children.\n",
            "Design a virtual reality headset that can be used in a museum.\n",
            "Design a virtual reality headset for a medical simulation.\n",
            "Design a virtual reality experience for a theme park.\n",
            "Create a virtual reality experience for a cultural event.\n",
            "Design a virtual reality experience for a museum.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Digite a pergunta do produto (ou 'sair' para encerrar): Tutu Ballet\n",
            "\n",
            "Pergunta Realizada: Tutu Ballet \n",
            "\n",
            "\n",
            "Resposta do modelo 1:\n",
            "\n",
            "Tutu Ballet Wrap Skirt.\n",
            "100% Polyester Wrap Skirt. 98% Polyester, 2% Spandex. Fits true to size.\n",
            "This Tutu Ballet Wrap Skirt is the perfect addition to your little girl's dance costumes! With a beautiful wrap skirt and attached tulle tutu, this skirt is perfect for any ballet or tap performance! The wrap skirt is made from 98% polyester, 2% spandex for stretch. It fits true to size.\n",
            "\n",
            "Resposta do modelo 2:\n",
            "\n",
            "Tutu Ballet Pointe Shoes\n",
            "Designer: Tutu Ballet Pointe Shoes\n",
            "\n",
            "Description: \n",
            "Tutu Ballet Pointe Shoes are the perfect combination of comfort and fashion. These ballet pointe shoes are designed for dancers who need a stylish shoe that also offers exceptional support. The high arched heel adds extra support and stability to the foot while the breathable upper fabric allows for maximum ventilation. The lightweight synthetic sole is flexible, and durable and the rubber toe cap provides added protection. The shoe's strap is adjustable for a custom fit, and the Tutu Ballet Pointe Shoes have a\n",
            "\n",
            "==================================================\n",
            "\n",
            "Digite a pergunta do produto (ou 'sair' para encerrar): Stuffed Plush Flying Duck Costume Party Hat With Sounds\n",
            "\n",
            "Pergunta Realizada: Stuffed Plush Flying Duck Costume Party Hat With Sounds \n",
            "\n",
            "\n",
            "Resposta do modelo 1:\n",
            "\n",
            "Stuffed Plush Flying Duck Costume Party Hat With Sounds - 100% Polyester. Size: 60 cm (2ft 3in). It's a great way to add some whimsical charm and playfulness to any costume or party.\n",
            "- Flying Ducks: \"Flying Ducks\" Party Costumes. Flying ducks are sure to bring a smile to your face as they swoop past you, making their way to your destination!\n",
            "- The Flying Duck Party Hat. This Flying Duck Party Hat is available in multiple colors, making it a perfect gift for any duck lover.\n",
            "- A Flying D\n",
            "\n",
            "Resposta do modelo 2:\n",
            "\n",
            "Stuffed Plush Flying Duck Costume Party Hat With Sounds.\n",
            "Despite its name, this hat is more suitable for adults.\n",
            "The hat features a plush duck design and is designed with sound makers to produce the noise of a real duck.\n",
            "It is made of a soft, sturdy material that is comfortable to wear.\n",
            "The sound makers are hidden inside the hat's chin strap so it is easy to put on and take off.\n",
            "The hat is perfect for parties, family events, and costume parties.\n",
            "It is a great addition to the duck costume, but it can also be worn as a regular hat.\n",
            "The sound mak\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Considerações Finais do Projeto\n",
        "Objetivo cumprido: Foi realizado com sucesso o fine-tuning de um foundation model (TinyLlama) para responder perguntas baseadas no título dos produtos da Amazon, utilizando o dataset AmazonTitles-1.3MM.\n",
        "\n",
        "Processo estruturado: O pipeline completo foi implementado, desde a montagem do ambiente e carregamento dos dados, passando pela preparação, filtragem e formatação dos exemplos para fine-tuning.\n",
        "\n",
        "Uso de LoRA: A técnica LoRA permitiu realizar fine-tuning eficiente e rápido em uma pequena amostra do dataset, reduzindo o custo computacional sem perda significativa da qualidade do ajuste.\n",
        "\n",
        "Avaliação comparativa: O modelo fine-tunado foi comparado com a versão base, demonstrando melhorias na geração das respostas, mais alinhadas com as descrições reais dos produtos.\n",
        "\n",
        "Interface prática: Uma interface interativa simples foi criada para que usuários possam testar o modelo e obter respostas em tempo real, validando sua aplicabilidade.\n",
        "\n",
        "Próximos passos sugeridos: ampliar o volume de dados para treinar com mais exemplos, ajustar hiperparâmetros para melhorar a qualidade, explorar outras arquiteturas de foundation models, e eventualmente implementar uma API para uso mais robusto do modelo."
      ],
      "metadata": {
        "id": "k2dYknDG6cSC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u9nk3iwKOUOa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}