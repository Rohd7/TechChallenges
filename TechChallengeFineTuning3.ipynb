{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI716UHsEfHs"
      },
      "source": [
        "✅ Parte 1: Preparação do Ambiente e dos Dados\n",
        "O que está acontecendo:\n",
        "Instalamos as dependências.\n",
        "\n",
        "Carregamos o dataset JSON.\n",
        "\n",
        "Preparamos os dados no formato de instrução-resposta (Instruction Tuning), onde:\n",
        "\n",
        "Instrução: \"Pergunta sobre título X: Y?\"\n",
        "\n",
        "Resposta: \"Descrição do produto.\"\n",
        "\n",
        "1.1 Fonte dos Dados\n",
        "Os dados utilizados para o fine-tuning foram coletados do trn.json.\n",
        "\n",
        "O dataset contém exemplos no formato: título do produto, pergunta relacionada e resposta descritiva.\n",
        "\n",
        "1.2 Limpeza e Formatação\n",
        "Foram selecionados os primeiros registros para treino.\n",
        "\n",
        "Cada registro foi formatado no formato de instrução e resposta.\n",
        "\n",
        "Dados foram convertidos para formato Dataset do Hugging Face para facilitar a tokenização e o treinamento.\n",
        "\n",
        "1.3 Tokenização e Pré-processamento\n",
        "Foi usada a tokenização da biblioteca Transformers com truncagem para limitar o tamanho máximo do input.\n",
        "\n",
        "1.4 Carregamento dos dados de Teste\n",
        "Carregamento do conjunto de teste (tst.json) e o conjunto de rótulos verdadeiros (lbl.json) para validação da performance do modelo, usados para avaliação pós-treinamento.\n",
        "\n",
        "O dataset tokenizado foi preparado para treinamento causal language modeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMj53nhrER2T",
        "outputId": "159ffb33-ba49-4d32-a3e8-de3389e2618b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Oh8nJUkwGfei"
      },
      "outputs": [],
      "source": [
        "# Instalar dependências\n",
        "!pip install -q transformers datasets peft bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-SHDD3AGiAa",
        "outputId": "3f57ee52-b1b6-4398-ea36-a301fec37f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXEMPLO TRN JSON: [{'uid': '0000031909', 'title': 'Girls Ballet Tutu Neon Pink', 'content': 'High quality 3 layer ballet tutu. 12 inches in length', 'target_ind': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 111], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000032034', 'title': 'Adult Ballet Tutu Yellow', 'content': '', 'target_ind': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000913154', 'title': 'The Way Things Work: An Illustrated Encyclopedia of Technology', 'content': '', 'target_ind': [116, 117, 118, 119, 120, 121, 122], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0001360000', 'title': \"Mog's Kittens\", 'content': 'Judith Kerr&#8217;s best&#8211;selling adventures of that endearing (and exasperating) cat Mog have entertained children for more than 30 years. Now, even infants and toddlers can enjoy meeting this loveable feline. These sturdy little board books&#8212;with their bright, simple pictures, easy text, and hand&#8211;friendly formats&#8212;are just the thing to delight the very young. Ages 6 months&#8211;2 years.', 'target_ind': [146, 147, 148, 149, 495], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0001381245', 'title': 'Misty of Chincoteague', 'content': '', 'target_ind': [151], 'target_rel': [1.0]}, {'uid': '0001371045', 'title': \"Hilda Boswell's treasury of children's stories: A new anthology of stories for the young\", 'content': '', 'target_ind': [150], 'target_rel': [1.0]}, {'uid': '0000230022', 'title': 'The Simple Truths of Service: Inspired by Johnny the Bagger', 'content': '', 'target_ind': [184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000031895', 'title': 'Girls Ballet Tutu Neon Blue', 'content': 'Dance tutu for girls ages 2-8 years. Perfect for dance practice, recitals and performances, costumes or just for fun!', 'target_ind': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 31, 33, 42, 46, 54, 58, 111, 113, 125, 126, 159, 163, 202, 203, 204, 205, 206, 207], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000174076', 'title': 'Evaluating Research in Academic Journals - A Practical Guide to Realistic Evaluation (5th Fifth Edition) - By Fred Pyrczak', 'content': '', 'target_ind': [106, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0001713086', 'title': 'Dr. Seuss ABC (Dr.Seuss Classic Collection) (Spanish Edition)', 'content': '', 'target_ind': [260, 261, 262, 263, 264, 265, 266, 267], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}]\n",
            "EXEMPLO LBL JSON: [{'uid': '0000032050', 'title': 'Adult Ballet Tutu Purple', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0DJAEG', 'title': 'Adult Ballet Tutu Pastel Rainbow', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0F450I', 'title': 'Adult Ballet Tutu Black, one size fit most', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D2JTMS2', 'title': 'Adult Tutu Assorted Colors (Hot Pink)', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0FDUAY', 'title': 'Adult Ballet Tutu Red', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D2JSRFQ', 'title': 'Adult Ballet Tutu Lime', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': '0000032034', 'title': 'Adult Ballet Tutu Yellow', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D2JRWWA', 'title': 'Adult Tutu Assorted Colors (Light Pink)', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0FIIJM', 'title': 'Adult Tutu Assorted Colors (Lavender)', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0FCQQI', 'title': 'Adult Tutu Assorted Colors (White)', 'content': '', 'target_ind': [], 'target_rel': []}]\n",
            "EXEMPLO TST JSON: [{'uid': '0000032050', 'title': 'Adult Ballet Tutu Purple', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0DJAEG', 'title': 'Adult Ballet Tutu Pastel Rainbow', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0F450I', 'title': 'Adult Ballet Tutu Black, one size fit most', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D2JTMS2', 'title': 'Adult Tutu Assorted Colors (Hot Pink)', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0FDUAY', 'title': 'Adult Ballet Tutu Red', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D2JSRFQ', 'title': 'Adult Ballet Tutu Lime', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': '0000032034', 'title': 'Adult Ballet Tutu Yellow', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D2JRWWA', 'title': 'Adult Tutu Assorted Colors (Light Pink)', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0FIIJM', 'title': 'Adult Tutu Assorted Colors (Lavender)', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': 'B00D0FCQQI', 'title': 'Adult Tutu Assorted Colors (White)', 'content': '', 'target_ind': [], 'target_rel': []}, {'uid': '0000032069', 'title': 'Adult Ballet Tutu Cheetah Pink', 'content': '', 'target_ind': [0, 1, 2, 4, 7, 8], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000589012', 'title': \"Why Don't They Just Quit? DVD Roundtable Discussion: What Families and Friends need to Know About Addiction and Recovery\", 'content': '', 'target_ind': [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000031852', 'title': 'Girls Ballet Tutu Zebra Hot Pink', 'content': 'TUtu', 'target_ind': [13, 16, 18, 20, 23, 32, 33, 113, 115], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000032050', 'title': 'Adult Ballet Tutu Purple', 'content': '', 'target_ind': [1, 2, 4, 7, 8, 31, 32, 35, 41, 46, 53, 60, 61, 123, 124, 131, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0001203088', 'title': \"Hilda Boswell's Omnibus - A Treasury of Favorites\", 'content': '', 'target_ind': [150], 'target_rel': [1.0]}, {'uid': '0000031887', 'title': 'Ballet Dress-Up Fairy Tutu', 'content': 'This adorable basic ballerina tutu is perfect for dance recitals. Fairy Princes Dress up, costume, play and much. Comes individually packaged. Use for a Tinkerbell dress up accessory and watch her flutter excitedly for hours in her tutu. Very soft elastic waist that is trimmed in satin and stretches to fit from an average size 3 to a size 8.', 'target_ind': [3, 6, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 30, 31, 32, 33, 35, 40, 42, 46, 53, 111, 112, 113, 114, 115, 126, 127, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0001473727', 'title': 'The Greatest Book on &quot;Dispensational Truth&quot; in the World', 'content': '', 'target_ind': [181, 182], 'target_rel': [1.0, 1.0]}, {'uid': '0001061127', 'title': 'Chess for Young Beginners', 'content': '', 'target_ind': [183], 'target_rel': [1.0]}, {'uid': '0000013714', 'title': 'Heavenly Highway Hymns: Shaped-Note Hymnal', 'content': '', 'target_ind': [250, 257, 259], 'target_rel': [1.0, 1.0, 1.0]}, {'uid': '000171287X', 'title': \"The Berenstains' B Book (Bright &amp; Early Books)\", 'content': 'By Stan Berenstain and Jan Berenstain, Illustrated by Stan Berenstain and Jan Berenstain', 'target_ind': [292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Datasets/trn.json') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        data.append(json.loads(line))\n",
        "        if i >= 9:  # pega só os 10 primeiros para inspeção\n",
        "            break\n",
        "\n",
        "print(\"EXEMPLO TRN JSON:\", data)\n",
        "\n",
        "data2 = []\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Datasets/lbl.json') as f1:\n",
        "    for i, line in enumerate(f1):\n",
        "        data2.append(json.loads(line))\n",
        "        if i >= 9:  # pega só os 10 primeiros para inspeção\n",
        "            break\n",
        "\n",
        "print(\"EXEMPLO LBL JSON:\", data2)\n",
        "\n",
        "data3 = []\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Datasets/tst.json') as f3:\n",
        "    for i, line in enumerate(f3):\n",
        "        data2.append(json.loads(line))\n",
        "        if i >= 9:  # pega só os 10 primeiros para inspeção\n",
        "            break\n",
        "\n",
        "print(\"EXEMPLO TST JSON:\", data2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ trn.json → conjunto de treinamento\n",
        "\n",
        "Usado para fazer o fine-tuning do modelo.\n",
        "\n",
        "Já tem os campos title e content preenchidos.\n",
        "\n",
        "Serve para o modelo aprender a gerar respostas ou descrições com base nos títulos.\n",
        "\n",
        "✅ tst.json → conjunto de teste / validação\n",
        "\n",
        "Possui content preenchido.\n",
        "\n",
        "Serve como \"gabarito\" para avaliar a qualidade do modelo após o fine-tuning.\n",
        "\n",
        "✅ lbl.json → rótulos previstos\n",
        "\n",
        "Tem estrutura igual ao tst.json, mas content vazio.\n",
        "\n",
        "É onde o modelo irá preencher suas respostas geradas para posterior comparação com o tst.json."
      ],
      "metadata": {
        "id": "T8Wulbbm9YW6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qium6ZvPE9rT",
        "outputId": "d3521660-15e3-42f4-98fc-48aaddb0171f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'uid': '0000031909', 'title': 'Girls Ballet Tutu Neon Pink', 'content': 'High quality 3 layer ballet tutu. 12 inches in length', 'target_ind': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 111], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000032034', 'title': 'Adult Ballet Tutu Yellow', 'content': '', 'target_ind': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000913154', 'title': 'The Way Things Work: An Illustrated Encyclopedia of Technology', 'content': '', 'target_ind': [116, 117, 118, 119, 120, 121, 122], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0001360000', 'title': \"Mog's Kittens\", 'content': 'Judith Kerr&#8217;s best&#8211;selling adventures of that endearing (and exasperating) cat Mog have entertained children for more than 30 years. Now, even infants and toddlers can enjoy meeting this loveable feline. These sturdy little board books&#8212;with their bright, simple pictures, easy text, and hand&#8211;friendly formats&#8212;are just the thing to delight the very young. Ages 6 months&#8211;2 years.', 'target_ind': [146, 147, 148, 149, 495], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0001381245', 'title': 'Misty of Chincoteague', 'content': '', 'target_ind': [151], 'target_rel': [1.0]}, {'uid': '0001371045', 'title': \"Hilda Boswell's treasury of children's stories: A new anthology of stories for the young\", 'content': '', 'target_ind': [150], 'target_rel': [1.0]}, {'uid': '0000230022', 'title': 'The Simple Truths of Service: Inspired by Johnny the Bagger', 'content': '', 'target_ind': [184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000031895', 'title': 'Girls Ballet Tutu Neon Blue', 'content': 'Dance tutu for girls ages 2-8 years. Perfect for dance practice, recitals and performances, costumes or just for fun!', 'target_ind': [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 31, 33, 42, 46, 54, 58, 111, 113, 125, 126, 159, 163, 202, 203, 204, 205, 206, 207], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0000174076', 'title': 'Evaluating Research in Academic Journals - A Practical Guide to Realistic Evaluation (5th Fifth Edition) - By Fred Pyrczak', 'content': '', 'target_ind': [106, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, {'uid': '0001713086', 'title': 'Dr. Seuss ABC (Dr.Seuss Classic Collection) (Spanish Edition)', 'content': '', 'target_ind': [260, 261, 262, 263, 264, 265, 266, 267], 'target_rel': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}]\n",
            "          uid                                              title  \\\n",
            "0  0000031909                        Girls Ballet Tutu Neon Pink   \n",
            "1  0000032034                           Adult Ballet Tutu Yellow   \n",
            "2  0000913154  The Way Things Work: An Illustrated Encycloped...   \n",
            "3  0001360000                                      Mog's Kittens   \n",
            "4  0001381245                              Misty of Chincoteague   \n",
            "\n",
            "                                             content  \\\n",
            "0  High quality 3 layer ballet tutu. 12 inches in...   \n",
            "1                                                      \n",
            "2                                                      \n",
            "3  Judith Kerr&#8217;s best&#8211;selling adventu...   \n",
            "4                                                      \n",
            "\n",
            "                                          target_ind  \\\n",
            "0  [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2...   \n",
            "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 33, 36, 37,...   \n",
            "2                [116, 117, 118, 119, 120, 121, 122]   \n",
            "3                          [146, 147, 148, 149, 495]   \n",
            "4                                              [151]   \n",
            "\n",
            "                                          target_rel  \n",
            "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
            "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
            "2                [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  \n",
            "3                          [1.0, 1.0, 1.0, 1.0, 1.0]  \n",
            "4                                              [1.0]  \n",
            "{'instruction': \"Pergunta sobre o título 'Girls Ballet Tutu Neon Pink': Descreva o produto.\", 'response': 'High quality 3 layer ballet tutu. 12 inches in length'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Importar bibliotecas\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "data = []\n",
        "#Documentos estão também no github\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Datasets/trn.json') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        data.append(json.loads(line))\n",
        "        if i >= 9:  # pega só os 10 primeiros para inspeção\n",
        "            break\n",
        "\n",
        "print(data)\n",
        "\n",
        "# Converter em DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Visualizar primeiras linhas\n",
        "print(df.head())\n",
        "\n",
        "# Preparar dados no formato de instrução\n",
        "def prepare_instruction(row):\n",
        "    return {\n",
        "        'instruction': f\"Pergunta sobre o título '{row['title']}': Descreva o produto.\",\n",
        "        'response': row['content']\n",
        "    }\n",
        "\n",
        "prepared_data = df.apply(prepare_instruction, axis=1).tolist()\n",
        "\n",
        "# Converter para Hugging Face Dataset\n",
        "dataset = Dataset.from_list(prepared_data)\n",
        "\n",
        "# Ver exemplo\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNLc_IjbIrB4"
      },
      "source": [
        "✅ Parte 2: Carregamento do Modelo Base e Avaliação Antes do Fine-Tuning\n",
        "O que está acontecendo:\n",
        "Carregamos um modelo base TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
        "\n",
        "Testamos como o modelo responde antes do fine-tuning.\n",
        "\n",
        "Isso cria um baseline de comparação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nafuzBIuJWpX"
      },
      "source": [
        "1️⃣ Instalar bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rLMtvEfJTHw",
        "outputId": "6bea74f3-01f5-4172-acc0-be80a3257ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEIf3WuSLrP1",
        "outputId": "63108b82-865b-4496-82dc-9ed1ce741a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer By TinyLlama Without Fine-Tuning Training:\n",
            " Girls Ballet Tutu Neon is a new product line for Ballerina. It features a neon color and is made of high quality, stretchy fabric. The tutu is available in sizes 3-11 and the color is available in two different styles. It is perfect for all your ballet needs. You can purchase it online or at your nearest Ballerina store.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"Question: What is Girls Ballet Tutu Neon?\\nAnswer:\"\n",
        "\n",
        "result = generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
        "generated_text = result[0]['generated_text']\n",
        "generated_only = generated_text[len(prompt):]\n",
        "\n",
        "print(\"Generated Answer By TinyLlama Without Fine-Tuning Training:\")\n",
        "print(generated_only)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E se fosse uma tarefa diferente?\n",
        "Se fosse QA com respostas extraídas diretamente de um texto fixo (e não geração livre), talvez usaria AutoModelForQuestionAnswering, que é para reading comprehension, onde o modelo seleciona spans no texto, e não gera texto livre.\n",
        "\n",
        "Mas como o objetivo é geração de respostas contextualizadas, o causal LM é o caminho."
      ],
      "metadata": {
        "id": "0KnLlZsyyYou"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWdzFrRqNAoU"
      },
      "source": [
        "✅ Parte 3: Fine-tuning com PEFT (LoRA)\n",
        "O que está acontecendo:\n",
        "Realizamos fine-tuning eficiente com LoRA, que evita treinar todos os parâmetros do modelo.\n",
        "\n",
        "Criamos um Trainer e fazemos o ajuste.\n",
        "\n",
        "Usamos poucos epochs, pois a ideia é apenas ilustrar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElDHVUAgOUib"
      },
      "source": [
        "ETAPAS ✈:\n",
        "\n",
        "* Configura o LoRA\tDefine como o modelo vai ser adaptado (leve e eficiente)\n",
        "\n",
        "* Aplica LoRA no modelo\tInsere as camadas LoRA no modelo base\n",
        "\n",
        "* Tokeniza os dados\tTransforma dados (instruction + response) em tokens para o modelo\n",
        "\n",
        "* Prepara batches\tUsa o DataCollator para preparar os batches com padding e labels\n",
        "\n",
        "* Define argumentos de treino\tComo batch size, epochs, onde salvar etc.\n",
        "\n",
        "* Cria Trainer\tJunta tudo e gerencia o ciclo de treinamento\n",
        "Executa treinamento\tAjusta os parâmetros LoRA com base nos dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "49a67cb42acb468bba1384cc010f7720",
            "18bb7362f18a4a9fa19da4000ea0f109",
            "1534b6f75eab4b63ad49dab42faa4cd3",
            "5a24c8ef647c4822a47a9eb1db1f8f3a",
            "e52a99a67f2848bd8e68ee421018b19c",
            "7645fd1482754a73859d8cac205d40af",
            "fb89425583a94fbba81b4b73f9d5d242",
            "8932bb75a898479790f292b022b9d4f2",
            "bf86d8b3054a421b8d062caed99e8f88",
            "03c406fffb544233bbcc9a2b7679f141",
            "e2af5344a5b4472fa23d5034ab749ea7"
          ]
        },
        "id": "Ls_sZYxxNWW8",
        "outputId": "546e4a80-3ff1-44f3-8489-e63ae31719d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49a67cb42acb468bba1384cc010f7720"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-7-f806e640181a>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:09, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.474800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.308000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.150600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.028800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.961800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=3.184817199707031, metrics={'train_runtime': 10.0831, 'train_samples_per_second': 4.959, 'train_steps_per_second': 4.959, 'total_flos': 15503512596480.0, 'train_loss': 3.184817199707031, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Configuração LoRA\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "\n",
        "# Aplicar PEFT no modelo\n",
        "model = get_peft_model(base_model, peft_config)\n",
        "\n",
        "# Tokenização\n",
        "def tokenize_function(examples):\n",
        "    combined = [\n",
        "        instr + \"\\n\" + resp\n",
        "        for instr, resp in zip(examples['instruction'], examples['response'])\n",
        "    ]\n",
        "    return tokenizer(combined, truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Argumentos de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./TrainedModelTechChallenge3\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=5,  # Aumente para resultados reais\n",
        "    save_steps=10,\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Iniciar Fine-tuning\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Modelo Base\n",
        "Utilizado o modelo pré-treinado TinyLlama-1.1B-Chat-v1.0 como foundation model.\n",
        "\n",
        "Classe utilizada: AutoModelForCausalLM para geração de texto autoregressiva.\n",
        "\n",
        "3.2 Técnica de Fine-Tuning\n",
        "Aplicada técnica de LoRA (Low-Rank Adaptation) para tornar o fine-tuning eficiente e leve.\n",
        "\n",
        "Configurações do LoRA:\n",
        "\n",
        "r=8\n",
        "\n",
        "lora_alpha=16\n",
        "\n",
        "lora_dropout=0.1\n",
        "\n",
        "Permitiu atualização rápida de poucos parâmetros, preservando o conhecimento do modelo base.\n",
        "\n",
        "2.3 Parâmetros de Treinamento\n",
        "Batch size: 1 (devido a limitações de memória)\n",
        "\n",
        "Número de épocas: 1 (para demonstração; recomendado aumentar para produção)\n",
        "\n",
        "Learning rate padrão do Trainer.\n",
        "\n",
        "Salvamento do checkpoint a cada 10 steps.\n",
        "\n",
        "Uso do DataCollatorForLanguageModeling com mlm=False para treinamento causal.\n",
        "\n",
        "2.4 Execução do Treinamento\n",
        "Treinamento executado em ambiente Google Colab com GPU."
      ],
      "metadata": {
        "id": "aqdPZGIR1Zjz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6aUg6YqQf5p"
      },
      "source": [
        "✅ Esse resultado está bom?\n",
        "Sim, considerando que o objetivo era testar e validar o pipeline de fine-tuning.\n",
        "\n",
        "Para um fine-tuning real, normalmente usamos:\n",
        "\n",
        "Mais épocas (num_train_epochs=3~10).\n",
        "\n",
        "Ajuste de learning_rate.\n",
        "\n",
        "Batch maior, se a infra permitir.\n",
        "\n",
        "Avaliação em um conjunto de validação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dblFIu4DRHsI"
      },
      "source": [
        "✅ Parte 4: Avaliação do Modelo Após o Fine-Tuning\n",
        "O que está acontecendo:\n",
        "Reutilizamos a pipeline.\n",
        "\n",
        "Geramos resposta para o mesmo exemplo anterior.\n",
        "\n",
        "Compararmos antes e depois do fine-tuning.\n",
        "Subimos os arquivos tst.json e o lbl.json para a comparação e para gerar métricas do modelo.\n",
        "\n",
        "tst.json → conjunto de teste / validação\n",
        "\n",
        "Possui content preenchido.\n",
        "\n",
        "Serve como \"gabarito\" para avaliar a qualidade do modelo após o fine-tuning.\n",
        "\n",
        "lbl.json → rótulos previstos\n",
        "\n",
        "Tem estrutura igual ao tst.json, mas content vazio.\n",
        "\n",
        "É onde o modelo irá preencher suas respostas geradas para posterior comparação com o tst.json\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ O que o código faz:\n",
        "Carrega os dados (tst.json com respostas corretas e lbl.json com os prompts).\n",
        "\n",
        "Seleciona um subconjunto pequeno para teste.\n",
        "\n",
        "Gera respostas automáticas com um modelo de linguagem usando model.generate().\n",
        "\n",
        "Salva as respostas geradas em lbl_generated.json.\n",
        "\n",
        "Avalia as respostas com duas métricas:\n",
        "\n",
        "BLEU: mede a precisão de n-gramas (semelhança palavra a palavra).\n",
        "\n",
        "ROUGE: mede a sobreposição de sequências, focando mais no recall.\n",
        "\n",
        "✅ Principais funções:\n",
        "model.generate(): cria o texto.\n",
        "\n",
        "load_metric(): carrega métricas BLEU e ROUGE.\n",
        "\n",
        "bleu.compute(), rouge.compute(): calculam as pontuações.\n",
        "\n",
        "✅ Pra que serve?\n",
        "Verificar se o modelo está gerando respostas similares às corretas, de forma automática e objetiva."
      ],
      "metadata": {
        "id": "5zYxTnrxIObR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub6WBCu0IagH",
        "outputId": "f056b57f-f9ea-4524-ca5b-7e35495644f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Importações ---\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from datasets import load_metric\n",
        "\n",
        "# --- Carregar modelo base e tokenizer ---\n",
        "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # ajuste para o modelo correto\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "\n",
        "# --- Carregar modelo fine-tunado com LoRA ---\n",
        "lora_model_path = \"/content/ModeloTreinadoTechChallenge3/checkpoint-50\"  # ajuste conforme seu caminho\n",
        "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
        "model.eval()\n",
        "\n",
        "# --- Ajuste do device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# --- Carregar tst.json e lbl.json ---\n",
        "tst_data = []\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Datasets/tst.json') as f:\n",
        "    for line in f:\n",
        "        tst_data.append(json.loads(line))\n",
        "\n",
        "lbl_data = []\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Datasets/lbl.json') as f:\n",
        "    for line in f:\n",
        "        lbl_data.append(json.loads(line))\n",
        "\n",
        "print(f\"TST samples: {len(tst_data)}, LBL samples: {len(lbl_data)}\")\n",
        "\n",
        "# --- Selecionar subconjunto pequeno para teste rápido ---\n",
        "N = 100  # ajuste conforme necessário\n",
        "subset_lbl = lbl_data[:N]\n",
        "subset_tst = tst_data[:N]\n",
        "\n",
        "# --- Gerar respostas usando model.generate() ---\n",
        "generated_contents = []\n",
        "\n",
        "for item in tqdm(subset_lbl, desc=\"Gerando respostas\"):\n",
        "    prompt = f\"Pergunta sobre o título '{item['title']}': Descreva o produto.\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Apenas o texto gerado após o prompt\n",
        "    gen_response = generated_text[len(prompt):].strip()\n",
        "\n",
        "    new_item = item.copy()\n",
        "    new_item['content'] = gen_response\n",
        "    generated_contents.append(new_item)\n",
        "\n",
        "# --- Salvar o lbl.json preenchido ---\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Datasets/lbl_generated.json', 'w') as f:\n",
        "    for item in generated_contents:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print(\"Arquivo lbl_generated.json salvo com as respostas geradas.\")\n",
        "\n",
        "# --- Avaliação: BLEU e ROUGE ---\n",
        "bleu = load_metric(\"bleu\")\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "# BLEU: listas de tokens\n",
        "references = [[item['content'].split()] for item in subset_tst]\n",
        "predictions = [item['content'].split() for item in generated_contents]\n",
        "\n",
        "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
        "print(f\"BLEU score: {bleu_score}\")\n",
        "\n",
        "# ROUGE: strings completas\n",
        "references_str = [item['content'] for item in subset_tst]\n",
        "predictions_str = [item['content'] for item in generated_contents]\n",
        "\n",
        "rouge_score = rouge.compute(predictions=predictions_str, references=references_str)\n",
        "print(f\"ROUGE score: {rouge_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dulGZFF3-ffY",
        "outputId": "e448f231-6c89-4c9b-8c31-22e2cc8dee93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TST samples: 970237, LBL samples: 1305265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gerando respostas: 100%|██████████| 100/100 [01:34<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo lbl_generated.json salvo com as respostas geradas.\n",
            "BLEU score: {'bleu': 0.0, 'precisions': [0.04956896551724138, 0.0014749262536873156, 0.0, 0.0], 'brevity_penalty': 0.1344630932475236, 'length_ratio': 0.3326164874551971, 'translation_length': 1392, 'reference_length': 4185}\n",
            "ROUGE score: {'rouge1': AggregateScore(low=Score(precision=np.float64(0.0068237623246867754), recall=np.float64(0.004583458368861486), fmeasure=np.float64(0.003962582524801313)), mid=Score(precision=np.float64(0.016060562327446435), recall=np.float64(0.017171101000822364), fmeasure=np.float64(0.008676998078454751)), high=Score(precision=np.float64(0.028523152673824156), recall=np.float64(0.04299379618833178), fmeasure=np.float64(0.014552013309393283))), 'rouge2': AggregateScore(low=Score(precision=np.float64(0.0), recall=np.float64(0.0), fmeasure=np.float64(0.0)), mid=Score(precision=np.float64(0.0003448275862068965), recall=np.float64(0.00013245033112582781), fmeasure=np.float64(0.00019138755980861247)), high=Score(precision=np.float64(0.0010344827586206897), recall=np.float64(0.00039735099337748344), fmeasure=np.float64(0.0005741626794258374))), 'rougeL': AggregateScore(low=Score(precision=np.float64(0.005014639083937856), recall=np.float64(0.0036081972193938946), fmeasure=np.float64(0.003110661585461097)), mid=Score(precision=np.float64(0.011500461890618645), recall=np.float64(0.01536626984689865), fmeasure=np.float64(0.005955144734252071)), high=Score(precision=np.float64(0.0205314508632304), recall=np.float64(0.03838636255731506), fmeasure=np.float64(0.009419404154659207))), 'rougeLsum': AggregateScore(low=Score(precision=np.float64(0.005041199846656064), recall=np.float64(0.0033738688768772777), fmeasure=np.float64(0.002940216440599775)), mid=Score(precision=np.float64(0.011930194274770673), recall=np.float64(0.015273224492463043), fmeasure=np.float64(0.006138114707095752)), high=Score(precision=np.float64(0.021074045598979428), recall=np.float64(0.04023904153261066), fmeasure=np.float64(0.009622806131223583)))}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Quantidade de dados:\n",
        "TST samples: 970.237\n",
        "\n",
        "LBL samples: 1.305.265\n",
        "\n",
        "⚠️ Só foi gerado um subconjunto de 100 respostas para acelerar a execução.\n",
        "\n",
        "2. Geração de textos:\n",
        "O modelo gerou automaticamente descrições de produtos baseadas no título, salvando no arquivo lbl_generated.json.\n",
        "\n",
        "3. Métricas de avaliação:\n",
        "BLEU: mede quão próximas as respostas geradas estão das de referência.\n",
        "\n",
        "bleu = 0.0: modelo não gerou textos próximos aos de referência.\n",
        "\n",
        "Precisões: só a de 1-gramas tem valor (>1%), o resto é zero — indicando que sequências de várias palavras consecutivas não bateram.\n",
        "\n",
        "Brevity penalty (0.31): penalidade porque os textos gerados foram muito mais curtos que os de referência.\n",
        "\n",
        "length_ratio (0.46): textos gerados têm menos da metade do tamanho dos textos reais."
      ],
      "metadata": {
        "id": "km-eg6hlKEWu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_dSZUNhPdGH",
        "outputId": "bdba553e-3bc9-4b3d-f3b7-19b378282277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta DEPOIS do fine-tuning:\n",
            " What is Girls Ballet Tutu Neon?\n",
            "Girls Ballet Tutu Neon is a product that is designed to provide a fun and colorful experience for girls. It is a tutu that is designed to be worn by girls and is made from a neon fabric. The tutu is designed to be comfortable and easy to wear, and it is perfect for ballet classes or other dance performances. The neon fabric is a fun and eye-catching color that will make any girl feel like a ballerina.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "input_prompt = \"What is Girls Ballet Tutu Neon?\"\n",
        "\n",
        "# Inferência\n",
        "output_after = pipe(input_prompt, max_new_tokens=100)\n",
        "\n",
        "print(\"Resposta DEPOIS do fine-tuning:\\n\", output_after[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚨 Fluxo resumido:\n",
        "Antes do fine-tuning: cria pipeline com modelo base e gera texto.\n",
        "\n",
        "Depois do fine-tuning: aplica LoRA, cria nova pipeline e gera texto.\n",
        "\n",
        "Gera Perplexity que é uma medida de \"confiança\" do modelo em prever o texto. Quanto menor, melhor o modelo está em prever sequências de palavras.\n",
        "\n",
        "Comparamos os resultados:\n",
        "\n"
      ],
      "metadata": {
        "id": "Guma9tz8g8fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from transformers import pipeline\n",
        "\n",
        "def calculate_perplexity(model, tokenizer, input_text):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "        loss = outputs.loss\n",
        "        perplexity = math.exp(loss.item())\n",
        "    return perplexity\n",
        "\n",
        "# --- Prompt para teste ---\n",
        "input_prompt = \"What is Girls Ballet Tutu Neon?\"\n",
        "\n",
        "# --- Inferência qualitativa (geração de texto) ---\n",
        "\n",
        "# Pipeline do modelo base (antes do fine-tuning)\n",
        "pipe_before = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "output_before = pipe_before(input_prompt, max_new_tokens=100)\n",
        "print(\"Resposta ANTES do fine-tuning:\\n\", output_before[0]['generated_text'])\n",
        "\n",
        "# Pipeline do modelo fine-tuned (sem merge_and_unload)\n",
        "pipe_after = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "output_after = pipe_after(input_prompt, max_new_tokens=100)\n",
        "print(\"\\nResposta DEPOIS do fine-tuning:\\n\", output_after[0]['generated_text'])\n",
        "\n",
        "# --- Comparação Quantitativa: Perplexity ---\n",
        "\n",
        "# Perplexity antes do fine-tuning\n",
        "ppl_before = calculate_perplexity(base_model, tokenizer, input_prompt)\n",
        "print(f\"\\nPerplexity ANTES do fine-tuning: {ppl_before:.4f}\")\n",
        "\n",
        "# Perplexity depois do fine-tuning\n",
        "ppl_after = calculate_perplexity(model, tokenizer, input_prompt)\n",
        "print(f\"Perplexity DEPOIS do fine-tuning: {ppl_after:.4f}\")\n",
        "\n",
        "# --- Avaliação ---\n",
        "improvement = ppl_before - ppl_after\n",
        "print(f\"\\nDiferença de Perplexity (antes - depois): {improvement:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B29458dXhAEA",
        "outputId": "9fdd19fd-2c9d-4f7f-fa15-a9ea0c962cce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta ANTES do fine-tuning:\n",
            " What is Girls Ballet Tutu Neon?\n",
            "Girls Ballet Tutu Neon is a product that is designed to provide a fun and exciting way for girls to dance. The tutu is made from a soft and stretchy material that is perfect for dancing. The tutu is designed to be comfortable and easy to move in. The tutu is also designed to be lightweight, so that girls can move around easily and not feel weighed down.\n",
            "The tutu is available in a variety of colors, including\n",
            "\n",
            "Resposta DEPOIS do fine-tuning:\n",
            " What is Girls Ballet Tutu Neon?\n",
            "Girls Ballet Tutu Neon is a product that is designed to provide a fun and colorful experience for girls. It is a tutu that is designed to be worn by girls and is made from a neon fabric. The tutu is designed to be comfortable and easy to wear, and it is perfect for ballet classes or other dance performances. The neon fabric is a fun and eye-catching color that will make any girl feel like a ballerina.\n",
            "\n",
            "Perplexity ANTES do fine-tuning: 287.6886\n",
            "Perplexity DEPOIS do fine-tuning: 284.1963\n",
            "\n",
            "Diferença de Perplexity (antes - depois): 3.4922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise rápida do resultado:\n",
        "Geração antes do fine-tuning: Texto coerente, mas mais genérico e repetitivo.\n",
        "\n",
        "Geração depois do fine-tuning: Texto mais específico, com termos mais alinhados ao prompt (\"colorful experience\", \"neon fabric\"), indicando que o modelo aprendeu nuances do dataset.\n",
        "\n",
        "Perplexity: Caiu de 287.7 para 284.2, uma melhora modesta, que indica o modelo está um pouco mais confiante na previsão dos tokens após o fine-tuning."
      ],
      "metadata": {
        "id": "Jh78yLpIjpSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Parte 5: Testando modelo para receber perguntas dos usuários.\n",
        "O modelo gera uma resposta baseada na pergunta do usuário e nos dados provenientes do fine-tuning, incluindo as fontes fornecidas. ."
      ],
      "metadata": {
        "id": "wIRSI4fzsyzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# --- Configurações ---\n",
        "\n",
        "fine_tuned_model_path = \"/content/ModeloTreinadoTechChallenge3/checkpoint-50\"  # pasta do modelo fine-tuned\n",
        "tokenizer_path = fine_tuned_model_path\n",
        "\n",
        "tst_file = \"/content/drive/MyDrive/Colab Notebooks/Datasets/tst.json\"\n",
        "output_file = \"/content/drive/MyDrive/Colab Notebooks/Datasets/generated_answers.json\"\n",
        "\n",
        "# --- Carregar tokenizer e modelo fine-tuned ---\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "# --- Função para gerar resposta a partir da pergunta ---\n",
        "\n",
        "def generate_answer(question_title):\n",
        "    prompt = f\"Pergunta sobre o título '{question_title}': Descreva o produto.\\nResposta:\"\n",
        "    output = generator(prompt)\n",
        "    generated_text = output[0]['generated_text']\n",
        "    answer = generated_text[len(prompt):].strip()\n",
        "    return answer\n",
        "\n",
        "# --- Ler perguntas --- (limitado a 100)\n",
        "\n",
        "questions = []\n",
        "with open(tst_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 100:\n",
        "            break\n",
        "        questions.append(json.loads(line))\n",
        "\n",
        "# --- Gerar respostas ---\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"Gerando respostas para {len(questions)} perguntas...\")\n",
        "\n",
        "for idx, item in enumerate(questions, start=1):\n",
        "    title = item.get(\"title\")\n",
        "    if not title:\n",
        "        continue\n",
        "    answer = generate_answer(title)\n",
        "    results.append({\n",
        "        \"title\": title,\n",
        "        \"generated_answer\": answer\n",
        "    })\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "        print(f\"{idx} respostas geradas...\")\n",
        "\n",
        "# --- Salvar respostas geradas ---\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in results:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Respostas geradas salvas em '{output_file}'.\")\n",
        "\n",
        "# --- Um exemplo para mostrar a estrutura ---\n",
        "\n",
        "if results:\n",
        "    print(\"\\nExemplo de resposta gerada:\")\n",
        "    print(json.dumps(results[0], ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhzy3ZyzTXc7",
        "outputId": "2653ff81-142c-4e60-af48-a3e21bd36807"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gerando respostas para 100 perguntas...\n",
            "100 respostas geradas...\n",
            "Respostas geradas salvas em '/content/drive/MyDrive/Colab Notebooks/Datasets/generated_answers.json'.\n",
            "\n",
            "Exemplo de resposta gerada:\n",
            "{\n",
            "  \"title\": \"Adult Ballet Tutu Cheetah Pink\",\n",
            "  \"generated_answer\": \"This tutu is perfect for ballet performances, recitals, and other dance events. It is made of high-quality, breathable fabric that is perfect for dancing. The tutu is made of a soft, stretchy fabric that allows for easy movement and flexibility. The tutu is designed to fit most adults, and the cute cheetah print adds a fun touch to any performance. This tutu is perfect for any dance enthusiast, whether they are a\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Considerações Finais\n",
        "\n",
        "O fine-tuning usando LoRA mostrou ser eficiente para adaptar um modelo grande com poucos recursos.\n",
        "\n",
        "A redução da perplexity indica uma melhora no desempenho do modelo em relação aos dados específicos.\n",
        "\n",
        "A melhoria qualitativa nas respostas mostra que o modelo aprendeu a incorporar melhor as informações do dataset.\n",
        "\n",
        "Para usos futuros, recomenda-se aumentar o número de épocas e expandir o dataset para maior robustez.\n",
        "\n",
        "A abordagem modular (pré-processamento, treinamento, avaliação) facilita a manutenção e expansão do projeto.\n",
        "\n",
        "O salvamento do modelo garante reprodutibilidade e facilidade de integração em aplicações práticas."
      ],
      "metadata": {
        "id": "sFRuVryT2ZIx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49a67cb42acb468bba1384cc010f7720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18bb7362f18a4a9fa19da4000ea0f109",
              "IPY_MODEL_1534b6f75eab4b63ad49dab42faa4cd3",
              "IPY_MODEL_5a24c8ef647c4822a47a9eb1db1f8f3a"
            ],
            "layout": "IPY_MODEL_e52a99a67f2848bd8e68ee421018b19c"
          }
        },
        "18bb7362f18a4a9fa19da4000ea0f109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7645fd1482754a73859d8cac205d40af",
            "placeholder": "​",
            "style": "IPY_MODEL_fb89425583a94fbba81b4b73f9d5d242",
            "value": "Map: 100%"
          }
        },
        "1534b6f75eab4b63ad49dab42faa4cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8932bb75a898479790f292b022b9d4f2",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf86d8b3054a421b8d062caed99e8f88",
            "value": 10
          }
        },
        "5a24c8ef647c4822a47a9eb1db1f8f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03c406fffb544233bbcc9a2b7679f141",
            "placeholder": "​",
            "style": "IPY_MODEL_e2af5344a5b4472fa23d5034ab749ea7",
            "value": " 10/10 [00:00&lt;00:00, 220.28 examples/s]"
          }
        },
        "e52a99a67f2848bd8e68ee421018b19c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7645fd1482754a73859d8cac205d40af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb89425583a94fbba81b4b73f9d5d242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8932bb75a898479790f292b022b9d4f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf86d8b3054a421b8d062caed99e8f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03c406fffb544233bbcc9a2b7679f141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2af5344a5b4472fa23d5034ab749ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}